{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flask-chatterbot-simple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC9_kW-ZlS6W",
        "collapsed": true,
        "outputId": "2aca71b9-733a-4e34-eb44-71515e50ea49"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQv6NX-YlnAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaafdf6d-1d17-4f3b-8be5-53e7321164e6"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template, request\n",
        "import re\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, RepeatVector, concatenate, TimeDistributed\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from nltk.tokenize import casual_tokenize\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import pickle\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "# text clean up imports\n",
        "import textwrap\n",
        "import nltk.data\n",
        "\n",
        "# fold paths when using Colab\n",
        "TEMPLATE = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/templates'\n",
        "STATIC = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/static'\n",
        " \n",
        "#create flask app \n",
        "app = Flask(__name__,\n",
        "            template_folder=TEMPLATE,\n",
        "            static_folder=STATIC)\n",
        "\n",
        "# run with ngrok when using Colab\n",
        "run_with_ngrok(app)\n",
        "\n",
        "# model paths when using Colab\n",
        "seq2seq_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/seq2seq'\n",
        "intents_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/data/intents'\n",
        "models_path = '/content/drive/MyDrive/Colab Notebooks/chatbot-flask-simple/models'\n",
        "\n",
        "class chatbot:\n",
        "    def __init__(self):\n",
        "        self.max_vocab_size = 50000\n",
        "        self.max_seq_len = 30\n",
        "        self.embedding_dim = 100\n",
        "        self.hidden_state_dim = 100\n",
        "        self.epochs = 80\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-4\n",
        "        self.dropout = 0.3\n",
        "        self.data_path = r'G:\\My Drive\\chatbot\\twcs.csv'\n",
        "        self.outpath = seq2seq_path\n",
        "        self.version = 'v1'\n",
        "        self.mode = 'inference'\n",
        "        self.num_train_records = 50000\n",
        "        self.load_model_from = os.path.join(seq2seq_path, 's2s_model_v1_.h5')\n",
        "        self.vocabulary_path = os.path.join(seq2seq_path, 'vocabulary.pkl')\n",
        "        self.reverse_vocabulary_path = os.path.join(seq2seq_path, 'reverse_vocabulary.pkl')\n",
        "        self.count_vectorizer_path = os.path.join(seq2seq_path, 'count_vectorizer.pkl')\n",
        "        self.t_path = os.path.join(intents_path, 'tokenizer.pickle')\n",
        "        self.UNK = 0\n",
        "        self.PAD = 1\n",
        "        self.START = 2\n",
        "\n",
        "        # intent model variables\n",
        "        #update method of predict call when updating model\n",
        "        self.intent_load_model_from = os.path.join(intents_path, 'pretrained_embeddings.h5')\n",
        "        self.intent_load_intents_from = os.path.join(intents_path, 'intents_job_intents.json')\n",
        "        self.intent_load_classes = os.path.join(intents_path, 'intents_classes.pkl')\n",
        "        self.intent_load_words = os.path.join(intents_path, 'intents_words.pkl')\n",
        "\n",
        "    def process_data(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        if self.mode =='train':\n",
        "            data = pd.read_csv(path)\n",
        "            data['in_response_to_tweet_id'].fillna(-12345, inplace=True)\n",
        "            tweets_in = data[data['in_response_to_tweet_id'] == -12345]\n",
        "            tweets_in_out = tweets_in.merge(data, left_on=['tweet_id'], right_on=['in_response_to_tweet_id'])\n",
        "            return tweets_in_out[:self.num_train_records]\n",
        "        elif self.mode == 'inference':\n",
        "            return data\n",
        "\n",
        "    def replace_anonymized_names(self, data):\n",
        "\n",
        "        def replace_name(match):\n",
        "            cname = match.group(2).lower()\n",
        "            if not cname.isnumeric():\n",
        "                return match.group(1) + match.group(2)\n",
        "            return '@__cname__'\n",
        "\n",
        "            re_pattern = re.compile('(@|Y@)([a-zA-Z0-9_]+)')\n",
        "            if self.mode == 'train':\n",
        "                in_text = data['text_x'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n",
        "                out_text = data['text_y'].apply(lambda txt: re_pattern.sub(replace_name, txt))\n",
        "                return list(in_text.values), list(out_text.values)\n",
        "            else:\n",
        "                return list(map(lambda x: re_pattern.sub(replace_name, x), data))\n",
        "\n",
        "    def tokenize_text(self, in_text, out_text):\n",
        "        count_vectorizer = CountVectorizer(tokenizer=casual_tokenize, max_features=self.max_vocab_size - 3)\n",
        "        count_vectorizer.fit(in_text + out_text)\n",
        "        self.analyzer = count_vectorizer.build_analyzer()\n",
        "        self.vocabulary = {key_: value_ + 3 for key_, value_ in count_vectorizer.vocabulary_.items()}\n",
        "        self.vocabulary['UNK'] = self.UNK\n",
        "        self.vocabulary['PAD'] = self.PAD\n",
        "        self.vocabulary['START'] = self.START\n",
        "        self.reverse_vocabulary = {value_: key_ for key_, value_ in self.vocabulary.items()}\n",
        "        joblib.dump(self.vocabulary, self.outpath + 'vocabulary.pkl')\n",
        "        joblib.dump(self.reverse_vocabulary, self.outpath + 'reverse_vocabulary.pkl')\n",
        "        joblib.dump(count_vectorizer, self.outpath + 'count_vectorizer.pkl')\n",
        "\n",
        "    def words_to_indices(self, sent):\n",
        "        word_indices = [self.vocabulary.get(token, self.UNK) for token in self.analyzer(sent)] + [self.PAD] * self.max_seq_len\n",
        "        word_indices = word_indices[:self.max_seq_len]\n",
        "        return word_indices\n",
        "\n",
        "    def indices_to_words(self, indices):\n",
        "        return ' '.join(self.reverse_vocabulary[id] for id in indices if id != self.PAD).strip()\n",
        "\n",
        "    def data_transform(self, in_text, out_text):\n",
        "        X = [self.words_to_indices(s) for s in in_text]\n",
        "        Y = [self.words_to_indices(s) for s in out_text]\n",
        "        return np.array(X), np.array(Y)\n",
        "\n",
        "    def train_test_split_(self, X, Y):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
        "        y_train = y_train[:, :, np.newaxis]\n",
        "        y_test = y_test[:, :, np.newaxis]\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def data_creation(self):\n",
        "        data = self.process_data(self.data_path)\n",
        "        in_text, out_text = self.replace_anonymized_names(data)\n",
        "        test_sentences = []\n",
        "        test_indexes = np.random.randint(1, self.num_train_records, 10)\n",
        "        for ind in test_indexes:\n",
        "            sent = in_text[ind]\n",
        "            test_sentences.append(sent)\n",
        "        self.tokenize_text(in_text, out_text)\n",
        "        X, Y = self.data_transform(in_text, out_text)\n",
        "        X_train, X_test, y_train, y_test = self.train_test_split_(X, Y)\n",
        "        return X_train, X_test, y_train, y_test, test_sentences\n",
        "\n",
        "    def define_model(self):\n",
        "\n",
        "        # Embedding Layer\n",
        "        embedding = Embedding(\n",
        "            output_dim=self.embedding_dim,\n",
        "            input_dim=self.max_vocab_size,\n",
        "            input_length=self.max_seq_len,\n",
        "            name='embedding',\n",
        "        )\n",
        "        # Encoder input\n",
        "        encoder_input = Input(\n",
        "            shape=(self.max_seq_len,),\n",
        "            dtype='int32',\n",
        "            name='encoder_input',\n",
        "        )\n",
        "        embedded_input = embedding(encoder_input)\n",
        "\n",
        "        encoder_rnn = LSTM(\n",
        "            self.hidden_state_dim,\n",
        "            name='encoder',\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        # Context is repeated to the max sequence length so that the same context\n",
        "        # can be feed at each step of decoder\n",
        "        context = RepeatVector(self.max_seq_len)(encoder_rnn(embedded_input))\n",
        "\n",
        "        # Decoder\n",
        "        last_word_input = Input(\n",
        "            shape=(self.max_seq_len,),\n",
        "            dtype='int32',\n",
        "            name='last_word_input',\n",
        "        )\n",
        "\n",
        "        embedded_last_word = embedding(last_word_input)\n",
        "        # Combines the context produced by the encoder and the last word uttered as inputs\n",
        "        # to the decoder.\n",
        "\n",
        "        decoder_input = concatenate([embedded_last_word, context], axis=2)\n",
        "\n",
        "        # return_sequences causes LSTM to produce one output per timestep instead of one at the\n",
        "        # end of the input, which is important for sequence producing models.\n",
        "        decoder_rnn = LSTM(\n",
        "            self.hidden_state_dim,\n",
        "            name='decoder',\n",
        "            return_sequences=True,\n",
        "            dropout=self.dropout\n",
        "        )\n",
        "\n",
        "        decoder_output = decoder_rnn(decoder_input)\n",
        "\n",
        "        #TimeDistributed allows the dense layer to be applied to each decoder output per timestep\n",
        "        next_word_dense = TimeDistributed(\n",
        "            Dense(int(self.max_vocab_size / 20), activation='relu'),\n",
        "            name='next_word_dense',\n",
        "        )(decoder_output)\n",
        "\n",
        "        next_word = TimeDistributed(\n",
        "            Dense(self.max_vocab_size, activation='softmax'),\n",
        "            name='next_word_softmax'\n",
        "        )(next_word_dense)\n",
        "\n",
        "        return Model(inputs=[encoder_input, last_word_input], outputs=[next_word])\n",
        "\n",
        "    def create_model(self):\n",
        "        _model_ = self.define_model()\n",
        "        adam = Adam(learning_rate=self.learning_rate, clipvalue=5.0)\n",
        "        _model_.compile(optimizer=adam, loss='sparse_categorical_crossentropy')\n",
        "        return _model_\n",
        "\n",
        "    # Function to append the START indext to the response Y\n",
        "    def include_start_token(self, Y):\n",
        "        print(Y.shape)\n",
        "        Y = Y.reshape((Y.shape[0], Y.shape[1]))\n",
        "        Y = np.hstack((self.START * np.ones((Y.shape[0], 1)), Y[:, :-1]))\n",
        "        # Y = Y[:,:,np.newaxis]\n",
        "        return Y\n",
        "\n",
        "    def binarize_output_response(self, Y):\n",
        "        return np.array([np_utils.to_categorical(row, num_classes=self.max_vocab_size)\n",
        "                        for row in Y])\n",
        "\n",
        "    def respond_to_input(self, model, input_sent):\n",
        "        input_y = self.include_start_token(self.PAD *np.ones((1, self.max_seq_len)))\n",
        "        ids = np.array(self.words_to_indices(input_sent)).reshape((1, self.max_seq_len))\n",
        "        for pos in range(self.max_seq_len - 1):\n",
        "            pred = model.predict([ids, input_y]).argmax(axis=2)[0]\n",
        "            # pred = model.predict([ids, input_y])[0]\n",
        "            input_y[:, pos + 1] = pred[pos]\n",
        "        return self.indices_to_words(model.predict([ids, input_y]).argmax(axis=2)[0])\n",
        "\n",
        "    def train_model(self, model, X_train, X_test, y_train, y_test):\n",
        "        input_y_train = self.include_start_token(y_train)\n",
        "        print(input_y_train.shape)\n",
        "        input_y_test = self.include_start_token(y_test)\n",
        "        print(input_y_test.shape)\n",
        "        early = EarlyStopping(monitor='val_loss', patience=10, mode='auto')\n",
        "\n",
        "        checkpoint = ModelCheckpoint(self.outpath + 's2s_model_' + str(self.version) + '_.h5', monitor='val_loss',\n",
        "                                     verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "        lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0, mode='auto')\n",
        "\n",
        "        model.fit([X_train, input_y_train], y_train,\n",
        "                   epochs=self.epochs,\n",
        "                   batch_size=self.batch_size,\n",
        "                   validation_data=([X_test, input_y_test], y_test),\n",
        "                   callbacks=[early, checkpoint, lr_reduce],\n",
        "                   shuffle=True)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def generate_response(self, model, sentences):\n",
        "        output_responses = []\n",
        "        print(sentences)\n",
        "        for sent in sentences:\n",
        "            response = self.respond_to_input(model, sent)\n",
        "            output_responses.append(response)\n",
        "        out_df = pd.DataFrame()\n",
        "        out_df['Tweet in'] = sentences\n",
        "        out_df['Tweet out'] = output_responses\n",
        "        return out_df\n",
        "\n",
        "    def convert_to_sequence(self, sentence):\n",
        "        print(f'Sentence 2: {sentence}')\n",
        "        print(f'Sentence list: {[sentence]}')\n",
        "        sequence = self.tkizer.texts_to_sequences([sentence])\n",
        "        \n",
        "        print(f'Initial Tokenization: {sequence}')\n",
        "        sequence = pad_sequences(sequence, maxlen=25)\n",
        "        print\n",
        "        #sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "        return sequence\n",
        "\n",
        "    # return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "    def word_embedding(self, sentence, intent_words, show_details=True):\n",
        "        # tokenize the pattern\n",
        "        # intent words = all words\n",
        "        print(f'Sentence 1: {sentence}')\n",
        "        sequence = self.convert_to_sequence(sentence)\n",
        "        # bag of words - matrix of N words, vocabulary matrix\n",
        "\n",
        "        return(sequence)\n",
        "    \n",
        "    def clean_up_sentence(self, sentence):\n",
        "        sentence_words = nltk.word_tokenize(sentence)\n",
        "        sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "        return sentence_words\n",
        "\n",
        "    # return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "    def bow(self, sentence, intent_words, show_details=True):\n",
        "        # tokenize the pattern\n",
        "        sentence_words = self.clean_up_sentence(sentence)\n",
        "        # bag of words - matrix of N words, vocabulary matrix\n",
        "        bag = [0]*len(intent_words)\n",
        "        for s in sentence_words:\n",
        "            for i,w in enumerate(intent_words):\n",
        "                if w == s:\n",
        "                    # assign 1 if current word is in the vocabulary position\n",
        "                    bag[i] = 1\n",
        "                    if show_details:\n",
        "                        print('found in bag: %s' % w)\n",
        "        return(np.array(bag))\n",
        "\n",
        "    def predict_class(self, sentence, model, method):\n",
        "        if method == 'WE':\n",
        "            # filter predictions below a threshold\n",
        "            # sentence is usertext\n",
        "            # intent words are all the words\n",
        "            print(f'sentence: {sentence}')\n",
        "            sequence = self.word_embedding(sentence, self.intent_words, show_details=False)\n",
        "            print(f'final sequence: {sequence}')\n",
        "            res = model.predict(np.array(sequence))[0]\n",
        "            print(f'res: {res}')\n",
        "            ERROR_THRESHOLD = 0.25\n",
        "            results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "            print(f'results: {results}')\n",
        "            # sort by strength of probability\n",
        "            results.sort(key=lambda x: x[1], reverse=True)\n",
        "            return_list = []\n",
        "            print(f'return_list: {return_list}')\n",
        "            for r in results:\n",
        "                return_list.append({'intent': self.intent_classes[r[0]], 'probability': str(r[1])})\n",
        "            print(f'return_list: {return_list}')\n",
        "            return return_list\n",
        "        \n",
        "        if method == 'BOW':\n",
        "            # filter predictions below a threshold\n",
        "            p = self.bow(sentence, self.intent_words, show_details=False)\n",
        "            res = model.predict(np.array([p]))[0]\n",
        "            ERROR_THRESHOLD = 0.25\n",
        "            results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "            # sort by strength of probability\n",
        "            results.sort(key=lambda x: x[1], reverse=True)\n",
        "            return_list = []\n",
        "            for r in results:\n",
        "                return_list.append({'intent': self.intent_classes[r[0]], 'probability': str(r[1])})\n",
        "            return return_list\n",
        "\n",
        "\n",
        "    def getResponse(self, ints, intents_json):\n",
        "        tag = ints[0]['intent']\n",
        "        list_of_intents = intents_json['intents']\n",
        "        for i in list_of_intents:\n",
        "            if(i['tag'] == tag):\n",
        "                result = random.choice(i['responses'])\n",
        "                break\n",
        "            else:\n",
        "                result = 'I do not understand. Please input a different message.'\n",
        "        return result\n",
        "\n",
        "    def string_clean(self, response_orig):\n",
        "\n",
        "        def upper_repl(match):\n",
        "            punctuated_inits = \\\n",
        "                '-' + match.group(1).upper() + '.' \\\n",
        "                     + match.group(2).upper() + '.'\n",
        "            return punctuated_inits\n",
        "\n",
        "        response = response_orig\n",
        "        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        # remove '@__cname__'\n",
        "        response = response.replace('@__cname__ ', '')\n",
        "        \n",
        "        # remove spaces before punctuation\n",
        "        response = re.sub(r'\\s([,?.!\"](?:\\s|$))', r'\\1', response)\n",
        "        # tokenize sentences\n",
        "        sentences = sent_tokenizer.tokenize(response)\n",
        "        # captialize senteces\n",
        "        sentences = [sent.capitalize() for sent in sentences]\n",
        "\n",
        "        # add html formatting\n",
        "        sentences = '</span><br><span>'.join(sentences)\n",
        "        sentences += '</span>'\n",
        "        # capitalize DM\n",
        "        sentences = sentences.replace('dm', 'dm'.upper())\n",
        "\n",
        "        # replace '^' with '-'\n",
        "        sentences = sentences.replace('^', '-')\n",
        "        pattern = re.compile(r'- \\b([a-z])([a-z])\\b')\n",
        "\n",
        "        sentences = re.sub(pattern, upper_repl, sentences)\n",
        "        return sentences\n",
        "\n",
        "    def main(self):\n",
        "        if self.mode == 'train':\n",
        "            X_train, X_test, y_train, y_test, test_sentences = self.data_creation()\n",
        "            print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "            print('Data Creation completed')\n",
        "            model = self.create_model()\n",
        "            print('Model creation completed')\n",
        "            model = self.train_model(model, X_train, X_test, y_train, y_test)\n",
        "            test_responses = self.generate_response(model, test_sentences)\n",
        "            print(test_sentences)\n",
        "            print(test_responses)\n",
        "            pd.DataFrame(test_responses).to_csv(self.outpath + 'output_response.csv', index=False)\n",
        "     \n",
        "        elif self.mode == 'inference':\n",
        "            #seq2seq model\n",
        "            model = load_model(self.load_model_from)\n",
        "            self.vocabulary = joblib.load(os.path.join(self.outpath, 'vocabulary.pkl'))\n",
        "            self.reverse_vocabulary = joblib.load(os.path.join(self.outpath, 'reverse_vocabulary.pkl'))\n",
        "            count_vectorizer = joblib.load(os.path.join(self.outpath, 'count_vectorizer.pkl'))\n",
        "            self.analyzer = count_vectorizer.build_analyzer()\n",
        "\n",
        "            #load intent model\n",
        "            intent_model = load_model(self.intent_load_model_from)\n",
        "            self.intent_intents = json.loads(open(self.intent_load_intents_from, encoding='cp1252').read())\n",
        "            self.intent_words = pickle.load(open(self.intent_load_words,'rb'))\n",
        "            self.intent_classes = pickle.load(open(self.intent_load_classes,'rb'))\n",
        "            self.tkizer = pickle.load(open(self.t_path,'rb'))\n",
        "\n",
        "            while True:\n",
        "                try:\n",
        "                    userText = request.args.get('msg')\n",
        "                    ints = self.predict_class(userText, intent_model, method='WE')\n",
        "                    intent_response = self.getResponse(ints, self.intent_intents)\n",
        "                    if (intent_response != 'help'):\n",
        "                        return str(intent_response)\n",
        "                    elif (intent_response == 'help'):\n",
        "                        response = self.respond_to_input(model, userText)\n",
        "                        response = self.string_clean(response)\n",
        "                        return str(response)\n",
        "\n",
        "                except(KeyboardInterrupt, EOFError, SystemExit):\n",
        "                    break\n",
        "\n",
        "        \n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    obj = chatbot()\n",
        "    obj.mode = 'inference'\n",
        "    response = obj.main()\n",
        "    return response\n",
        "\n",
        "app.run()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://8e44-34-83-10-34.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [09/Oct/2021 23:33:27] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Oct/2021 23:33:28] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Oct/2021 23:33:28] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Oct/2021 23:33:28] \"\u001b[37mGET /static/style.css HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [09/Oct/2021 23:33:30] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence: I have a product I need to return.\n",
            "Sentence 1: I have a product I need to return.\n",
            "Sentence 2: I have a product I need to return.\n",
            "Sentence list: ['I have a product I need to return.']\n",
            "Initial Tokenization: [[2, 18, 5, 9, 2, 10, 8, 13]]\n",
            "final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2 18  5  9  2 10  8\n",
            "  13]]\n",
            "res: [1.0721831e-08 4.7136487e-06 1.0693065e-10 1.3153005e-05 9.9997211e-01\n",
            " 3.4750799e-06 6.5848330e-06 2.1779105e-13]\n",
            "results: [[4, 0.9999721]]\n",
            "return_list: []\n",
            "return_list: [{'intent': 'Product Return', 'probability': '0.9999721'}]\n",
            "(1, 30)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [09/Oct/2021 23:33:46] \"\u001b[37mGET /get?msg=I%20have%20a%20product%20I%20need%20to%20return. HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence: What is your name?\n",
            "Sentence 1: What is your name?\n",
            "Sentence 2: What is your name?\n",
            "Sentence list: ['What is your name?']\n",
            "Initial Tokenization: [[15, 7, 6, 12]]\n",
            "final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 15  7  6\n",
            "  12]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [09/Oct/2021 23:34:01] \"\u001b[37mGET /get?msg=What%20is%20your%20name%3F HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res: [1.0615569e-05 9.9922502e-01 2.6465008e-07 7.3933235e-04 1.6874210e-05\n",
            " 1.6222687e-06 4.7784188e-06 1.5316253e-06]\n",
            "results: [[1, 0.999225]]\n",
            "return_list: []\n",
            "return_list: [{'intent': 'Name', 'probability': '0.999225'}]\n",
            "sentence: CAn you help me track my package?\n",
            "Sentence 1: CAn you help me track my package?\n",
            "Sentence 2: CAn you help me track my package?\n",
            "Sentence list: ['CAn you help me track my package?']\n",
            "Initial Tokenization: [[52, 1, 20, 17, 34, 4, 24]]\n",
            "final sequence: [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 52  1 20 17 34  4\n",
            "  24]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [09/Oct/2021 23:35:00] \"\u001b[37mGET /get?msg=CAn%20you%20help%20me%20track%20my%20package%3F HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res: [2.9656985e-05 4.3884997e-06 1.9988303e-11 1.3222405e-05 1.8427299e-05\n",
            " 9.9992549e-01 7.6962788e-06 9.7387681e-07]\n",
            "results: [[5, 0.9999255]]\n",
            "return_list: []\n",
            "return_list: [{'intent': 'Track Package', 'probability': '0.9999255'}]\n",
            "sentence: 4323098734323837432\n",
            "Sentence 1: 4323098734323837432\n",
            "Sentence 2: 4323098734323837432\n",
            "Sentence list: ['4323098734323837432']\n",
            "Initial Tokenization: [[]]\n",
            "final sequence: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [09/Oct/2021 23:35:14] \"\u001b[37mGET /get?msg=4323098734323837432 HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res: [5.0195675e-05 1.2791422e-05 9.9939191e-01 6.0815248e-05 1.7012933e-05\n",
            " 2.6345300e-05 1.7525459e-04 2.6570255e-04]\n",
            "results: [[2, 0.9993919]]\n",
            "return_list: []\n",
            "return_list: [{'intent': 'Number', 'probability': '0.9993919'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FSek_u97lkg"
      },
      "source": [
        ""
      ]
    }
  ]
}